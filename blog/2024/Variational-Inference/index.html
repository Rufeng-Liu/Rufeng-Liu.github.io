<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Variational Inference | Rufeng Liu </title> <meta name="author" content="Rufeng Liu"> <meta name="description" content="Variational Bayesian"> <meta name="keywords" content="Bayesian nonparametric, Model selection, Statistical computing"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/myfavicon.png?b20f02ad5e865d6bb37e8673b87bf49e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rufeng-liu.github.io/blog/2024/Variational-Inference/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Variational Inference",
            "description": "Variational Bayesian",
            "published": "June 28, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rufeng</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Variational Inference</h1> <p>Variational Bayesian</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#exponential-families">Exponential families</a> </div> <div> <a href="#dirichlet-process-and-dirichlet-process-mixture">Dirichlet process and Dirichlet process mixture</a> </div> <div> <a href="#inference">Inference</a> </div> <ul> <li> <a href="#gibbs-sampling">Gibbs sampling</a> </li> <li> <a href="#variational-inference">Variational inference</a> </li> </ul> <div> <a href="#implementation">Implementation</a> </div> </nav> </d-contents> <p>Use optimization rather than use sampling. First posit a family of densities, then to find a member of that family which is close to the target density. Use exponential family as an example.</p> <h2 id="exponential-families">Exponential families</h2> <p><a href="https://en.wikipedia.org/wiki/Exponential_family" rel="external nofollow noopener" target="_blank">Exponential families</a> include <a href="https://en.wikipedia.org/wiki/Normal_distribution" rel="external nofollow noopener" target="_blank">normal</a>, <a href="https://en.wikipedia.org/wiki/Log-normal_distribution" rel="external nofollow noopener" target="_blank">log-normal</a>, <a href="https://en.wikipedia.org/wiki/Exponential_distribution" rel="external nofollow noopener" target="_blank">exponential</a>, <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution" rel="external nofollow noopener" target="_blank">inverse Gaussian</a>, <a href="https://en.wikipedia.org/wiki/Gamma_distribution" rel="external nofollow noopener" target="_blank">gamma</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution" rel="external nofollow noopener" target="_blank">chi-squared</a>, <a href="https://en.wikipedia.org/wiki/Beta_distribution" rel="external nofollow noopener" target="_blank">beta</a>, <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="external nofollow noopener" target="_blank">Dirichlet</a>, <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="external nofollow noopener" target="_blank">Bernoulli</a>, <a href="https://en.wikipedia.org/wiki/Categorical_distribution" rel="external nofollow noopener" target="_blank">categorical</a>, <a href="https://en.wikipedia.org/wiki/Poisson_distribution" rel="external nofollow noopener" target="_blank">Poisson</a>, <a href="https://en.wikipedia.org/wiki/Wishart_distribution" rel="external nofollow noopener" target="_blank">Wishart</a>, <a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution" rel="external nofollow noopener" target="_blank">inverse Wishart</a>, <a href="https://en.wikipedia.org/wiki/Geometric_distribution" rel="external nofollow noopener" target="_blank">geometric</a>, <a href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="external nofollow noopener" target="_blank">binomial</a>(with fixed number of trials), <a href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="external nofollow noopener" target="_blank">multinomial</a>(with fixed number of trials), <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution" rel="external nofollow noopener" target="_blank">negative binomial</a>(with fixed number of failures), <a href="https://en.wikipedia.org/wiki/Weibull_distribution" rel="external nofollow noopener" target="_blank">Weibull</a>(with fixed shape parameter)…</p> <p>For variable \(\boldsymbol{x}=(x_1,\ldots,x_k)^{T}\), a family of distributions with paramter \(\boldsymbol{\theta}\equiv (\theta_1,\ldots,\theta_s)^{T}\) is said to belong to an exponential family if the p.d.f (or p.m.f) can be written as</p> \[f_X(\boldsymbol{X}\mid\boldsymbol{\theta})=h(\boldsymbol{x})\text{exp}\left(\sum_{i=1}^{s} \eta_{i}(\boldsymbol{\theta})T_i(\boldsymbol{x})-A(\boldsymbol{\theta})\right)\] <p>or campactly</p> \[f_X(\boldsymbol{X}\mid\boldsymbol{\theta})=h(\boldsymbol{x})\text{exp}\left(\boldsymbol{\eta}(\boldsymbol{\theta})\cdot T(\boldsymbol{x})-A(\boldsymbol{\theta})\right)\] <p>The dimensions \(k\) of the random variable need not match the dimension \(d\) of the parameter vector, nor (in the case of a curved exponential function) the dimension \(s\) of the natural parameter \(\boldsymbol{\eta}\) and sufficient statistic \(T(\boldsymbol{x})\) .</p> <h2 id="dirichlet-process-and-dirichlet-process-mixture">Dirichlet process and Dirichlet process mixture</h2> <p>Citation <d-cite key="ferguson1973bayesian"></d-cite></p> <p>A <code class="language-plaintext highlighter-rouge">Dirichlet process</code> \(G\) is parameterized by a centering measure \(G_0\) and a positive presicion/scaling parameter \(\alpha\), if for all natural numbers \(k\) and \(k\)-partitions \(\{B_1,\ldots,B_k\}\):</p> \[\left(G(B_1),\ldots,G(B_k)\right)\sim \text{Dir}\left(\alpha G_0(B_1),\ldots,\alpha G_0(B_k)\right).\] <p>Suppose \(N\) random variables \(\eta_n\) are independently drawn from \(G\):</p> \[\begin{aligned} G\mid G_0,\alpha &amp;\sim \text{DP}(G_0,\alpha)\\ \eta_n &amp;\sim G, \quad n\in\{1,\ldots,N\}. \end{aligned}\] <p>Conditioning on \(n − 1\) draws, the \(n\)th value is, with positive probability, exactly equal to one of those draws:</p> \[p(\cdot\mid \eta_1,\ldots,\eta_{n-1})\propto \alpha G_0(\cdot)+\sum_{i=1}^{n-1} \delta_{\eta_i}(\cdot).\] <p>Thus, the variables \(\{\eta_1,\ldots,\eta_{n−1}\}\) are randomly partitioned according to which variables are equal to the same value, with the distribution of the partition obtained from a Polya urn scheme.</p> <p>Let \(\{\eta^{*}_{1},\ldots,\eta^{*}_{\lvert \boldsymbol{c} \rvert }\}\) denote the distinct values of \(\{\eta_1,\ldots,\eta _{n-1}\}\), let \(\boldsymbol{c} = \{c_1,...,c_ {n−1}\}\) be assignment variables such that \(\eta_i = \eta^*_ {c_i}\), and let \(\lvert\boldsymbol{c}\rvert\) denote the number of cells in the partition. The distribution of \(\eta_n\) follows the urn distribution:</p> \[\eta_n = \begin{cases} \eta^*_i &amp; \text{with prob.} \frac{|\lbrace j:c_j=i \rbrace|}{n-1+\alpha} \\ \eta, \eta\sim G_0 &amp; \text{with prob.} \frac{\alpha}{n-1+\alpha}, \end{cases}\] <p>where</p> \[|\{j : c_ {j}=i\}|\] <p>is the number of times the value \(\eta^{*}_{i}\) occurs in \(\{\eta_{1},\ldots,\eta_{n−1}\}\).</p> <p>Given Dirichlet process \(G\), a DP mixtures are densities \(p(x)=\int p(x, \eta)d\eta\), or there can be non-i.i.d observations \(x_n\overset{ind}{\sim}p_{n,G}(x)=\int p(x;\eta)dG(\eta)\), in terms of \(N\) latent variables \(\eta_1,\ldots,\eta_N\), the model can be written as</p> \[x_n\mid\eta_n\overset{ind}{\sim}p_n(x_n;\eta_n), \quad \eta_n\mid G\overset{i.i.d}{\sim}G, \quad G\mid G_0,\alpha \sim \text{DP}(G_0,\alpha)\] <p>Given a sample \(\{x_1,\ldots,x_N\}\) from a DP mixture, the predictive density is</p> \[p(x\mid x_1,\ldots,x_N,\alpha,G_0)=\int p(x\mid \eta)p(\eta\mid x_1,\ldots,x_N,\alpha,G_0)d\eta\] <p>which one can use MCMC to achieve posterior draws, together with posterior distribution \(p(\eta\mid x_1,\ldots,x_N,\alpha,G_0)\).</p> <p>The <code class="language-plaintext highlighter-rouge">stick-breaking</code> representation <d-cite key="sethuraman1994constructive"></d-cite> is widely used. Consider two infinite collections of independent random variables, \(V_i\sim\text{Beta}(1,\alpha)\) and \(\eta^*_i\sim G_0\), for \(i=\{1,2,\ldots\}\). The stick-breaking representation of \(G\) is as follows:</p> \[G=\sum_{i=1}^{\infty} \pi_i(\boldsymbol{v})\delta_{\eta^*_i}, \quad \pi_i(\boldsymbol{v})=v_{i} \prod_{j=1}^{i-1}(1-v_j)\] <p>In the DP mixture, the vector \(\pi(\boldsymbol{v})\) comprises the infinite vector of mixing proportions and \(\{\eta^*_1,\eta^*_2,\ldots\}\) are the atoms representing the mixture components. Let \(Z_n\) be an assignment variable of the mixture component with which the data point \(x_n\) is associated. The data can be described as arising from the following process:</p> <ol> <li>Draw \(V_i\sim \text{Beta}(1,\alpha), \quad i=\{1,2,\ldots\}\)</li> <li>Draw \(\eta^*_i\mid G_0\sim G_0, \quad \quad i=\{1,2,\ldots\}\)</li> <li> <p>For the \(n\)th data point:</p> <p>(a) Draw \(Z_n\mid \{v_1,v_2,\ldots\}\sim \text{Mult}(\pi(\boldsymbol{v}))\); (b) Draw \(X_n\mid z_n\sim p(x_n\mid \eta^*_{z_n})\)</p> </li> </ol> <p>Restrict the DP mixtures that the observable data are drawn from an exponential family distribution, and where the base distribution for the DP is the corresponding conjugate prior.</p> <p>The distribution of \(X_n\) conditional on \(Z_n\) and \({\eta^*_1,\eta^*_2,\ldots}\) is:</p> \[p(x_n\mid z_n,\eta^*_1,\eta^*_2,\ldots)=\prod_{i=1}^{\infty} \left(h(x_n) \text{exp} \left\{ {\eta^* _i}^T x_n-a(\eta^*_i) \right\} \right)^{\mathbf{1}\lbrack z_n=i \rbrack}\] <p>where \(a(\eta^*_i)\) is the appropriate cumulant function and it is assumed for simplicity that \(x\) is the sufficient statistic for the natural parameter \(\eta\).</p> <p>The vector of sufficient statistics of the corresponding conjugate family is \(({\eta^* _i}^T, -a(\eta^*_i) )^T\). The base distribution is:</p> \[p(\eta^*\mid \lambda) = h(\eta^*) \text{exp}\left\{\lambda_1^T \eta^* + \lambda_2 (-a(\eta^*))-a(\lambda)\right\}\] <p>where the hyperparameter \(\lambda\) is decomposed, such that \(\lambda_1\) contains the first \(\dim(\eta^*)\) components and \(\lambda_2\) is a scalar.</p> <h2 id="inference">Inference</h2> <p>Citation <d-cite key="blei2017variational"></d-cite></p> <h3 id="gibbs-sampling">Gibbs sampling</h3> <p>Review of the collapsed Gibbs sampler and blocked Gibbs sampler for DP mixtures. Blocked Gibbs sampler outshines collapsed Gibbs sampler when \(G_0\) is not conjugate.</p> <h4 id="collapesd-gibbs-sampling">Collapesd Gibbs sampling</h4> <p>The <code class="language-plaintext highlighter-rouge">collapsed Gibbs sampler</code> for a DP mixture with conjugate base distribution integrates out the random measure \(G\) and distinct parameter values \(\{\eta^{*}_{1},\ldots,\eta^{*}_{\lvert \boldsymbol{c} \rvert }\}\). The Markov chain is thus defined only on the latent partition \(\boldsymbol{c} = \{c_1,...,c_ {N}\}\), where \(\lvert\boldsymbol{c}\rvert\) denote the number of cells in the partition. The algorithm iteratively samples each assignment variable \(C_n\), for \(n\in \{1,\ldots,N\}\), conditional on the other cells in the partition, \(\boldsymbol{c_{-n}}\). The assignment \(C_n\) can be one of \(\lvert \boldsymbol{c_{-n}}\rvert +1\) values: either the \(n\)th data point is in a cell with other data points, or in a cell by itself.</p> <p>Exchangeability implies that \(C_n\) has the following multinomial distribution:</p> \[p(c_n=k\mid \boldsymbol{x},\boldsymbol{c}_ {-n},\lambda,\alpha)\propto p(x_n\mid \boldsymbol{x}_ {-n}, \boldsymbol{c}_ {-n}, c_n=k, \lambda)p(c_n=k\mid \boldsymbol{c}_ {-n}, \alpha)\] <p>The first term is a ratio of normalizing constants of the posterior distribution of the \(k\)th parameter, one including and one excluding the \(n\)th data point:</p> \[p(x_n\mid \boldsymbol{x}_ {-n}, \boldsymbol{c}_ {-n}, c_n=k, \lambda)=\frac{\text{exp}\lbrace a(\lambda_1+\sum_{m\neq n} \mathbf{1} \lbrack c_m =k \rbrack x_m +x_n, \lambda_2 +\sum_{m\neq n} \mathbf{1} \lbrack c_m =k \rbrack +1)\rbrace}{\text{exp}\lbrace a(\lambda_1+\sum_{m\neq n} \mathbf{1} \lbrack c_m =k \rbrack x_m, \lambda_2 +\sum_{m\neq n} \mathbf{1} \lbrack c_m =k \rbrack)\rbrace}\] <p>The second term is given by the Polya urn scheme:</p> \[p(c_n=k\mid \boldsymbol{c}_ {-n}, \alpha) = \begin{cases} \vert\lbrace j:c_{-n,j}=k \rbrace\vert &amp; \text{if } k \text{ is an existing cell in the partition}\\ \alpha &amp; \text{if } k \text{ is a new cell in the partition} \end{cases}\] <p>where \(\vert\lbrace j:c_{-n,j}=k \rbrace\vert\) denotes the number of data points in the kth cell of the partition \(\boldsymbol{c}_{-n}\).</p> <p>After the chain has reached stationary distribution, \(B\) samples \(\lbrace \boldsymbol{c}_1, \dots, \boldsymbol{c}_B \rbrace\) are collected to approximate the posterior. The approximate predictive distribution is an average of the predictive distributions across the Monte Carlo samples:</p> \[p(x_{N+1}\mid x_1, \ldots, x_N,\alpha,\lambda)=\frac{1}{N} \sum_{b=1}^{B} p(x_{N+1}\mid \boldsymbol{c}_b,\boldsymbol{x},\alpha,\lambda)\] <p>For a given sample, that distribution is:</p> \[p(x_{N+1}\mid \boldsymbol{c}_ {b},\boldsymbol{x},\alpha,\lambda) = \sum_{k=1}^{\vert \boldsymbol{c}_ b \vert+1} p(c_{N+1}=k \mid \boldsymbol{c}_ b,\alpha)p(x_{N+1}\mid \boldsymbol{c}_ b, \boldsymbol{x}, c_{N+1}=k, \lambda)\] <h4 id="blocked-gibbs-sampling">Blocked Gibbs sampling</h4> <p>Ishwaran and James <d-cite key="ishwaran2001gibbs"></d-cite> developed a <code class="language-plaintext highlighter-rouge">blocked Gibbs sampling</code> algorithm based on the <code class="language-plaintext highlighter-rouge">stick-breaking</code> representation. A truncated Dirichlet process (TDP) is defined by setting \(v_{K-1}=1\) and \(\pi_i(\boldsymbol{v})=0\) for \(i\geq K\), and showed that the truncated process closely approximates a true Dirichlet process when the truncation level is chosen large relative to the number of data points.</p> <p>In the TDP mixture, the state of the Markov chain consists of the beta variables \(\boldsymbol{V}=\{V_1,\ldots,V_{K-1}\}\), the mixture component parameters \(\boldsymbol{\eta}^*=\{\eta_1^*,\ldots,\eta_K^*\}\), and the indicator variables \(\boldsymbol{Z}=\{Z_1,\ldots,Z_N\}\). The blocked Gibbs sampler iterates between the following three steps:</p> <ol> <li> <p>For \(n\in\{1,\ldots,N\}\), independently sample \(Z_n\) from:</p> \[p(z_n=k\mid \boldsymbol{v},\boldsymbol{\eta}^*,\boldsymbol{x}) = \pi_{k}(\boldsymbol{v})p(x_n\mid \eta^* _k)\] </li> <li> <p>For \(k\in\{1,\ldots,K\}\), independently sample \(V_k\) from \(\text{Beta}(\gamma_{k,1},\gamma_{k,2})\), where:</p> \[\begin{aligned} \gamma_{k,1} &amp; = 1 + \sum_{n=1}^{N} \mathbf{1}\lbrack z_n=k \rbrack \\ \gamma_{k,2} &amp; = \alpha + \sum_{i=k+1}^{K} \sum_{n=1}^{N} \mathbf{1}\lbrack z_n=i \rbrack \end{aligned}\] </li> <li> <p>For \(k\in\{1,\ldots,K\}\), independently sample \(\eta_k^*\) from \(p(\eta^ *_k \mid \tau_k)\). This distribution is in the same family as the base distribution, with parameters:</p> \[\begin{aligned} \tau_{k,1} &amp; = \lambda_1 + \sum_{i\neq n} \mathbf{1}\lbrack z_i=k \rbrack x_i\\ \tau_{k,2} &amp; = \lambda_2 + \sum_{i\neq n} \mathbf{1}\lbrack z_i=k \rbrack \end{aligned}\] <p>where as before, the hyperparameter \(\lambda\) of the conjugate exponential family is decomposeed, such that \(\lambda_1\) contains the first \(\dim(\eta^*)\) components and \(\lambda_2\) is a scalar.</p> </li> </ol> <p>After the chain has reached its stationary distribution, \(B\) samples are collected and an approximate predictive distribution can be constructed. For a particular sample:</p> \[p(x_{N+1}\mid \boldsymbol{z},\boldsymbol{x},\alpha,\lambda) = \sum_{k=1}^{K} \mathrm{E}\lbrack \pi_{i}(\boldsymbol{V} \mid \gamma_1,\ldots,\gamma_k) \rbrack p(x_{N+1}\mid \tau_k)\] <h3 id="variational-inference">Variational inference</h3> <p>Consider a model with hyperparameters \(\theta\), latent variables \(\boldsymbol{W}=\lbrace W_1,\ldots,W_M\rbrace\), and observations \(\boldsymbol{x}=\lbrace x_1,\ldots,x_N\rbrace\). The posterior distribution of the latent variables \(p(\boldsymbol{w}\mid\boldsymbol{x},\theta)=\frac{p(\boldsymbol{w},\boldsymbol{x}\mid \theta)}{p(\boldsymbol{x}\mid \theta)}=\text{exp}\lbrace\log p(\boldsymbol{w},\boldsymbol{x}\mid \theta)-\log p(\boldsymbol{x}\mid \theta)\rbrace\) is difficult to compute, because the latent variables become dependent when conditioning on observed data, then \(\log p(\boldsymbol{x}\mid \theta)=\log \int p(\boldsymbol{w},\boldsymbol{x}\mid \theta)d\boldsymbol{w}\) is hard to compute.</p> <p>A class of variational methods known as <code class="language-plaintext highlighter-rouge">mean-field methods</code> are based on optimizing Kullback-Leibler (KL) divergence with respect to a so-called variational distribution. Let \(q_{\nu}(\boldsymbol{w})\) be a family of distributions indexed by a variational parameter \(\nu\), the aim is to minimize the KL divergence between \(q_{\nu}(\boldsymbol{w})\) and \(p(\boldsymbol{w}\mid\boldsymbol{x},\theta)\):</p> \[\mathbf{D}(q_{\nu}(\boldsymbol{w}) \Vert p(\boldsymbol{w}\mid\boldsymbol{x},\theta)) = \mathbf{E}_ {q} \lbrack \log q_{\nu}(\boldsymbol{W}) \rbrack - \mathbf{E}_ {q} \lbrack \log p(\boldsymbol{W},\boldsymbol{x}\mid\theta) \rbrack + \log p(\boldsymbol{x}\mid \theta)\] <p>As the marginal probability does not depend on the variational parameters, it can be ignored in the optimization. To minimize the KL divergence can be cast alternatively as to compute the maximization of a lower bound on the log marginal likelihood:</p> \[\log p(\boldsymbol{x}\mid \theta) \geq \mathbf{E}_ {q} \lbrack \log p(\boldsymbol{W},\boldsymbol{x}\mid\theta) \rbrack - \mathbf{E}_ {q} \lbrack \log q_{\nu}(\boldsymbol{W}) \rbrack\] <p>To constructe the family \(q_{\nu}(\boldsymbol{w})\), it is in need to break some of dependencies between latent variables which make the true posterior difficult to compute.</p> <h4 id="mean-field-variational-inference-in-exponential-families">Mean field variational inference in exponential families</h4> <p>For each latent variable, assume that the conditional distribution is a member of the exponential family:</p> \[p(w_{i}\mid \boldsymbol{w}_ {-i},\boldsymbol{x},\theta)=h(w_{i})\text{exp} \lbrace {g_{i}(\boldsymbol{w}_ {-i},\boldsymbol{x},\theta)}^T w_{i} - a(g_{i}(\boldsymbol{w}_ {-i},\boldsymbol{x},\theta)) \rbrace\] <p>where \(g_{i}(\boldsymbol{w}_ {-i},\boldsymbol{x},\theta)\) is the natural parameter for \({w}_{i}\) when conditioning on the remaining latent variables and the observations.</p> <p>Consider the following family of distributions as mean field variational approximations:</p> \[q_{\boldsymbol{\nu}}(\boldsymbol{w})=\prod_{i=1}^{M} q_{\nu_i}(w_i) =\prod_{i=1}^{M} \text{exp} \lbrace \nu_{i}^T w_{i} - a(w_{i}) \rbrace\] <p>where \(\boldsymbol{\nu}=\lbrace \nu_1,\ldots, \nu_M \rbrace\) are variational parameters and each distribution is in the exponential family. Then it is shown that the optimization of KL divergence with respect to a single variational parameter \(\nu_{i}\) is achieved by computing the following expectation:</p> \[\nu_{i} = \mathbf{E}_ {q} \lbrack g_{i}(\boldsymbol{W}_ {-i},\boldsymbol{x},\theta) \rbrack\] <p>In a coordinate ascent algorithm, the bound with respect to each \(\nu_i\) is iteratively maximized, holding the other variational parameters fixed.</p> <p>Using the chain rule, the bound can be rewriten:</p> \[\log p(\boldsymbol{x}\mid \theta) \geq \log p(\boldsymbol{x}\mid \theta) + \sum_{m=1}^{M} \mathbf{E}_ {q} \lbrack \log p(W_m \mid W_1,\ldots, W_{m-1}, \boldsymbol{x},\theta) \rbrack - \sum_{m=1}^{M} \mathbf{E}_ {q} \lbrack \log q_{\nu_m}(W_m) \rbrack\] <p>To optimize with respect to \(\nu_{i}\), the part depend on \(\nu_{i}\) is :</p> \[l_{i}= \mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) \rbrack - \mathbf{E}_ {q} \lbrack \log q_{\nu_i}(W_i) \rbrack\] <p>Given that the variational distribution \(q_{\nu_i}(w_i)\) is in the exponential family:</p> \[q_{\nu_i}(w_i)=h(w_i)\text{exp}\lbrace \nu_i^T w_i - a(\nu_i) \rbrace\] <p>as in the exponential family \(\mathbf{E}_ {q} \lbrack W_i\rbrack = a'(\nu_i)\), it is easy to see,</p> \[\begin{aligned} l_{i} &amp;= \mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) - \log h(W_i) - \nu_i^T w_i + a(\nu_i) \rbrack \\ &amp;= \mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) \rbrack - \mathbf{E}_ {q} \lbrack \log h(W_i) \rbrack - \nu_i^T a'(\nu_i) + a(\nu_i) \end{aligned}\] <p>The derivative with respect to \(\nu_i\) is:</p> \[\frac{\partial l_{i}}{\partial \nu_i} = \frac{\partial}{\partial \nu_i} ( \mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) \rbrack - \mathbf{E}_ {q} \lbrack \log h(W_i) \rbrack ) - \nu_i^T a''(\nu_i)\] <p>let partial derivative equals \(0\), the optimal \(\nu_i\) satisfies:</p> \[\nu_i = \lbrack a''(\nu_i) \rbrack ^{-1} ( \frac{\partial}{\partial \nu_i}\mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) \rbrack - \frac{\partial}{\partial \nu_i}\mathbf{E}_ {q} \lbrack \log h(W_i) \rbrack )\] <p>as we assumed that the conditional distribution is a member of the exponential family:</p> \[p(w_{i}\mid \boldsymbol{w}_ {-i},\boldsymbol{x},\theta)=h(w_{i})\text{exp} \lbrace {g_{i}(\boldsymbol{w}_ {-i},\boldsymbol{x},\theta)}^T w_{i} - a(g_{i}(\boldsymbol{w}_ {-i},\boldsymbol{x},\theta)) \rbrace\] <p>where \(g_{i}(\boldsymbol{w}_ {-i},\boldsymbol{x},\theta)\) is the natural parameter for \(w_{i}\) when conditioning on the remaining latent variables and the observations. We have the expected log probability of \(W_i\) and its first derivative:</p> \[\begin{aligned} \mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) \rbrack &amp; = \mathbf{E}_ {q} \lbrack \log h(W_{i})\rbrack + \mathbf{E}_ {q} \lbrack g_{i}(\boldsymbol{W}_ {-i},\boldsymbol{x},\theta)\rbrack ^T a'(\nu_i) - \mathbf{E}_ {q} \lbrack a(g_{i}(\boldsymbol{W}_ {-i},\boldsymbol{x},\theta)) \rbrack \\ \frac{\partial}{\partial \nu_i} \mathbf{E}_ {q} \lbrack \log p(W_i \mid \boldsymbol{W}_ {-i}, \boldsymbol{x},\theta) \rbrack &amp; = \frac{\partial}{\partial \nu_i} \mathbf{E}_ {q} \lbrack \log h(W_{i})\rbrack + \mathbf{E}_ {q} \lbrack g_{i}(\boldsymbol{W}_ {-i},\boldsymbol{x},\theta)\rbrack^T a''(\nu_i) \end{aligned}\] <p>put first derivative in the equation of the optimal \(\nu_i\):</p> \[\nu_{i} = \mathbf{E}_ {q} \lbrack g_{i}(\boldsymbol{W}_ {-i},\boldsymbol{x},\theta) \rbrack\] <h4 id="coordinate-ascent-algorithm-for-dp-mixtures">Coordinate ascent algorithm for DP mixtures</h4> <p>Based on the stick-breaking representation, the latent variables are the stick lengths, the atoms, and the cluster assignments: \(\boldsymbol{W}=\{\boldsymbol{V},\boldsymbol{\eta}^*,\boldsymbol{Z}\}\), with the scaling parameter and the parameter of the conjugate base distribution \(\theta = \{\alpha,\lambda\}\) as the hyperparameters. The <code class="language-plaintext highlighter-rouge">variational bound</code> on the log marginal probability of the data:</p> \[\begin{aligned} \log p(\boldsymbol{x}\mid \alpha,\lambda) &amp; \geq \mathbf{E}_ {q} \lbrack \log p(\boldsymbol{V},\boldsymbol{\eta}^*,\boldsymbol{Z},\boldsymbol{x}\mid \alpha,\lambda) \rbrack - \mathbf{E}_ {q} \lbrack \log q(\boldsymbol{V},\boldsymbol{\eta}^*,\boldsymbol{Z}) \rbrack \\ &amp; = \sum_{n=1}^{N} (\mathbf{E}_ {q} \lbrack \log p(x_n \mid Z_n) \rbrack + \mathbf{E}_ {q} \lbrack \log p(Z_n \mid \boldsymbol{V}) \rbrack ) + \mathbf{E}_ {q} \lbrack \log p(\boldsymbol{V} \mid \alpha) \rbrack + \mathbf{E}_ {q} \lbrack \log p(\boldsymbol{\eta}^* \mid \lambda) \rbrack - \mathbf{E}_ {q} \lbrack \log q(\boldsymbol{V},\boldsymbol{\eta}^*,\boldsymbol{Z}) \rbrack \end{aligned}\] <p>The variational distribution is truncated at level \(T\), the factorized family of variational distributions for mean field variational inference is:</p> \[q(\boldsymbol{V},\boldsymbol{\eta}^*,\boldsymbol{Z})= \prod_{t=1}^{T-1} q_{\gamma_{t}}(\nu_t) \prod_{t=1}^{T} q_{\tau_{t}}(\eta_t^*) \prod_{n=1}^{N} q_{\phi_{n}}(z_n)\] <p>where \(q_{\gamma_{t}}(\nu_t)\) are beta distributions, \(q_{\tau_{t}}(\eta_t^*)\) are exponential family distributions with natural parameters \(\tau_{t}\), and \(q_{\phi_{n}}(z_n)\) are multinomial distributions. The free variational parameters are:</p> \[\boldsymbol{\nu} = \{ \gamma_{1}, \ldots, \gamma_{T-1}, \tau_{1}, \ldots, \tau_{T}, \phi_{1}, \ldots, \phi_{N}\}\] <p>In the <code class="language-plaintext highlighter-rouge">variational bound</code> on the log marginal probability of the data, \(\mathbf{E}_ {q} \lbrack \log p(Z_n \mid \boldsymbol{V}) \rbrack\) is the only term not involving standard computations in the exponential family, which can be rewriten using indicator random variables:</p> \[\begin{aligned} \mathbf{E}_ {q} \lbrack \log p(Z_n \mid \boldsymbol{V}) \rbrack &amp;= \mathbf{E}_ {q} \lbrack \log (\prod_{i=1}^{\infty} (1-V_i)^{\mathbf{1}\lbrack Z_n&gt;i \rbrack} V_{i}^{\mathbf{1}\lbrack Z_n=i \rbrack} ) \rbrack \\ &amp; = \prod_{i=1}^{\infty} q(z_n&gt;i) \mathbf{E}_ {q} \lbrack \log (1-V_i) \rbrack + q(z_n=i) \mathbf{E}_ {q} \lbrack \log V_i \rbrack \end{aligned}\] <p>Truncated at \(t=T\)</p> \[\mathbf{E}_ {q} \lbrack \log p(Z_n \mid \boldsymbol{V}) \rbrack = \prod_{i=1}^{T} q(z_n&gt;i) \mathbf{E}_ {q} \lbrack \log (1-V_i) \rbrack + q(z_n=i) \mathbf{E}_ {q} \lbrack \log V_i \rbrack\] <p>where</p> \[\begin{aligned} q(z_n=i) &amp;= \phi_{n,i} \\ q(z_n&gt;i) &amp;= \sum_{j=i+1}^{T} \phi_{n,j} \\ \mathbf{E}_ {q} \lbrack \log V_i \rbrack &amp;= \Psi(\gamma_{i,1}) - \Psi(\gamma_{i,1}+\gamma_{i,2})\\ \mathbf{E}_ {q} \lbrack \log (1-V_i) \rbrack &amp;= \Psi(\gamma_{i,2}) - \Psi(\gamma_{i,1}+\gamma_{i,2}) \end{aligned}\] <p>The digamma function \(\Psi\) is derivative of the log-partition (\(A(\eta)\)) in the beta distribution.</p> <p>From the conclusion in last subsection \(\nu_{i} = \mathbf{E}_ {q} \lbrack g_{i}(\boldsymbol{W}_ {-i},\boldsymbol{x},\theta) \rbrack\), a mean-field coordinate ascent algorithm is derived:</p> \[\begin{aligned} \gamma_{t,1} &amp;= 1 + \sum_{n} \phi_{n,t} \\ \gamma_{t,2} &amp;= \alpha + \sum_{n} \sum_{j=t+1}^{T} \phi_{n,t} \\ \tau_{t,1} &amp;= \lambda_1 + \sum_{n} \phi_{n,t} x_n \\ \tau_{t,2} &amp;= \lambda_1 + \sum_{n} \phi_{n,t} \\ \phi_{n,t} &amp;\propto \text{exp}(S_t) \end{aligned}\] <p>for \(t \in\{1, \ldots,T\}, n\in \{1, \ldots, N\}\), where</p> \[S_t = \mathbf{E}_ {q} \lbrack \log V_t \rbrack + \sum_{i=1}^{t-1} \mathbf{E}_ {q} \lbrack \log (1-V_i) \rbrack + \mathbf{E}_ {q} \lbrack \eta_{t}^* \rbrack ^T X_n - \mathbf{E}_ {q} \lbrack a(\eta_{t}^*) \rbrack\] <p>Iterating these updates optimizes <code class="language-plaintext highlighter-rouge">variational bound</code> with respect to the variational parameters in the factorized family of variational distributions \(\mathbf{E}_ {q} \lbrack \log p(Z_n \mid \boldsymbol{V}) \rbrack\).</p> <p>The predictive distribution is:</p> \[p(x_{N+1}\mid \boldsymbol{x}, \alpha, \lambda) = \int (\sum_{t=1}^{\infty} \pi_t(\boldsymbol{V}) p(x_{N+1}\mid \eta_{t}^*)) d P(\boldsymbol{v},\boldsymbol{\eta}^*\mid \boldsymbol{x}, \alpha, \lambda)\] <p>it is approximated with a product of expectations:</p> \[p(x_{N+1}\mid \boldsymbol{x}, \alpha, \lambda) \approx \sum_{t=1}^{T} \mathbf{E}_ {q} \lbrack \pi_t(\boldsymbol{V}) \rbrack \mathbf{E}_ {q} \lbrack p(x_{N+1}\mid \eta_{t}^*) \rbrack\] <p>where \(q\) depends on \(\boldsymbol{x}, \alpha, \lambda\).</p> <h2 id="implementation">Implementation</h2> <p>Citation <d-cite key="blei2017variational"></d-cite> <d-cite key="blei2006variational"></d-cite></p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-06-28-Variational-Inference.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Rufeng Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 24, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>